绪论
a)	背景
随着移动互联网、社交网络、物联网等新兴应用的发展，云计算成为IT领域的第三次浪潮，正在从根本上改变商业模式和产品，大量的传统行业被颠覆，在BBC纪录片“地平线：大数据时代”中提到，大数据正在被应用在治安、金融、外太空探索等等领域，扮演着重要的角色。
2008年谷歌日平均处理数据量已达20PB，而沃尔玛每小时产生100多万笔交易信息；按Teradata的数据，波音737客机一次6小时飞行产生传感器数据达240TB［1］云计算技术的应用促进了数据的转换、存储和处理的能力，也使得从数据中抽取有价值的信息变的容易。越来越多的智能终端和传感器被接入网络，数据量正在急速地增长并变的更加多样化。
随之而来的是两个明显的变化[2]。首先，所有的数据都可以被储存下来。这意味着需要数据积累以便实施的应用将更容易被实施。其次，随之而来的是从数据短缺到数据洪流的变化，这将给数据处理工具带来新的挑战，因为根据大数据的特性，需要有高扩展性的数据分析技术，新的数据表示方法。
从商业角度来看， Amazon在2010年11月29日销售的峰值达到了158笔，如果真想要搞清楚在每笔订单中究竟发生了什么，将需要非常大容量并且十分详细的数据，这其中有很多数据从未被接入过商务智能（BI）或者数据仓库（DW）中。同时，这些数据来自多种多样的传感器、智能终端、第三方应用和社交网络，所以数据不仅仅是数量巨大而且异常地多样化并高速地产生着。面对这一情况，我们可以继续使用BI报告或者数据仓库来处理数据，也可以使用大数据分析工具来发现数据中隐藏的规律。根据TDWI的报告，已经有部分企业开始部署适合大数据分析的企业数据仓库，也有企业选择分离数据仓库创建单独的分布式数据分析平台，也有企业选择了混合型的处理方案。其中现在高达64％的企业选择第一种方案，即在企业数据仓库中部署大数据分析；值得注意的是，在未来，30％的企业更倾向使用分布式文件系统（如Hadoop项目）或供应商大数据分析工具或云分析平台。[4]
 

这一趋势催生了很多新兴技术的发展，各类应用层出不穷。但从本质上来说，“大数据”不是一个学科或者技术的定义，而是一种现象的描述，是数据规模增长的现状下一种综合解决方案。Gartner所绘制的大数据技术成熟度曲线中发现，大数据正在快速进入高峰期，大量技术已经得到充分的发展或者已达到成熟（Fig.1），所以在大数据生态系统中，“大数据的重点不在于对数据的传输、收集、存储，而是重在对数据的分析挖掘，并由此获得凭直觉难以发现的有用信息。”[3] 
 
b)	意义。
在大数据中，由于数据重复（Data Duplication）、连续性相关（SeriesCorrelation）和数据互关联（Cross-Correlation）等特性[5]，存在着大量的无用的数据，即大数据的价值具有稀疏的特性。
普林斯顿大学的著名教授George A. Miller估算过，人类能够从长期记忆中即时调用并处理的记忆只有7±2个片段，这大约是3 bits，远少于1byte。所以虽然这些数据的冗余有助于寻找到真正有价值的信息，但是对于人脑而言则是无论如何都无法粹读的。[6]
同时，大数据分析，特别是高级分析（Advanced Analytics, a.k.a. Discovery Analytics）着重于发现不同于BI报告的前人所不知道的商业规律，作为知识发现的一个重要环节，在企业中的作用以及地位不断提高，数据科学家（Data Scientists）也成为了如同明星一般拥有光环的职业。
本文将阐述大数据分析的定义、特性、工具、分析方法，并结合实例介绍大数据分析的流程。同时讨论大数据分析的一些国内外的研究与应用现状，以及机遇与挑战，以便能够对大数据分析能有一个清晰的理解。

c)	结构。
绪论。简单介绍了本文的出现背景与研究意义，解释了数据爆炸时代的相关现象，所引发的变化和出现的相关技术，为大数据分析的出现和研究的意义做了必要的论证。
基础理论。深入浅出地介绍了大数据以及机器学习等等涉及到的相关概念与技术。以便能够对大数据分析有一个清楚的认识。
大数据分析。根据国内外文献以及技术文档，对大数据分析技术以及实际运用进行总结，分析大数据分析发展的前景与挑战以及具体应用场景。
应用。以＊＊＊为例解释了从数据到价值的整个分析过程和步骤和其中具体实现的方法以作参考。
总结。

2.	基础理论
大数据分析是高级分析技术与大数据处理技术的结合，通过大数据处理工具分析海量的非结构化和半结构化数据。其中高级分析技术主要包括预测分析（Predictive Analytics）、数据挖掘（Data mining）、机器学习（Machine Learning）、统计分析（Statistics）、人工智能（Artificial Intelligence）、自然语言处理（Nature Language Process）等等，上述的分析技术相互交叉也相互融合，而无法避免地会涉及到机器学习的各类算法和模型。本章节将简单介绍和大数据分析相关的基础理论，主要涉及大数据处理技术和机器学习基础理论。
a)	大数据
i.	概述。
大数据作为当今商界、学术界热点，其定义不断地被反复提及，但至今仍没有一个公认的定义，一般都是从大数据的特征出发来概括总结，其中比较有代表性的是从3个V出发的定义[7]，满足以下的三个特性：规模性(volume)、多样性(variety)以及高速性(velocity)。此外，还有部分定义在前三个V之上增加了第四个V。关于第四个V的说法有很多，国际数据公司（International Data Corporation,IDC）认为第四个V指代价值性（value）[8]，而IBM 公司认为大数据需要具有真实性（veracity）[9] 。从另一个角度来说，大数据是指利用常用软件工具捕获、管理和处理数据所耗时间超过可容忍时间的数据集[10]，而大数据这一现象的产生，最主要的原因是数据存储硬件与数据获得变得越来越容易。
规模性（volume）。由于存储数据的成本越来越便宜，全球越来越多的数据正在被记录和存储，随处可见的移动设备和无线传感器时时刻刻正在产生着大量的数据，每分钟有数以亿记的互联网设备进行着数据交换。2011年，国际数据公司（International Data Corporation）发表报告称，每两年全世界的数据将翻一番。2011年，全世界产生了1.8ZB的数据，相比去年增长了0.6ZB，到2020年，全世界将会产生高达35ZB的数据[11]，而这将会给数据存储带来显著的挑战。
多样性（variety）。大数据多由结构化数据、非结构化数据以及半结构化数据构成，而且三者会不停地相互转化。值得注意的是，结构化的数据只占到全部数据的20％，其他的80％都为非结构化或半结构化的数据，这包括了互联网上由用户产生的数据、社交网络产生的数据交换以及物联网设备产生的数据[12]。
高速性（velocity）。高速性指的是高频、实时的数据处理以及近乎实时发生的大数据分析。传统商务智能和数据仓库中，实时的需求显然较为低下，而随着数据爆炸，海量的数据需要及时地被分析处理。
价值性（value）。在企业中通常使用结构化数据来进行统计分析。而大数据需要分析人员从海量地数据中提取出有价值地信息来预测未来趋势和进行决策支持。此外，大数据还具有价值稀疏性，例如大量的监视器视频被存储下来，而真正有用的也就数秒钟的画面。
ii.	处理模式。
大数据处理一般可以分为两种：流处理（stream processing）和批处理（batch processing）。批处理是先存储数据后处理的模式，而流处理则是直接对数据进行处理。
1.	流处理。
流处理应用的思想是数据的价值会根据时间流逝而减少，所以快速处理最新的数据成为流处理应用共同的目标。采用流处理方式处理大数据的主要应用场景常常为网页分析、传感器网络和金融高频交易等。
下图为典型的流处理模型，所有的处理过程基本在内存中完成，所以内存容量是流处理发展主要的瓶颈所在。流处理的理论和技术已经有10多年的发展历史，不少实践应用也得到了部署和使用。Twitter的Storm [20]、Yahoo的S4[21]以及Linkedin的Kafka[22]是其中比较有代表性的开源系统。
 
2.	批处理。
批处理模式是一种在大型集群上处理分布式数据集的编程模式，以谷歌2004年提出的MapReduce编程模式使用最为广泛，2012年Apache发布了MapReduce v2（YARN），加强了对资源的管理，减少了资源的消耗，各部分的功能更加专一，本文中仍以MapReduce进行介绍。在众多讨论和文献中，MapReduce一直作为Hadoop项目的核心框架而被反复讨论。总的来说，Mapreduce将分析推向数据，而非移动大量的数据来实现分析。[13]MapReduce模型主要通过Map阶段和Reduce阶段来处理链／值（key/value）对应的数据。如下图，是MapReduce处理模式的流程概括：
 
Map阶段：首先将数据集进行分割，并将每一个分割的部分指派给任务追踪器（task tracker）来执行Map程序，并对数据进行处理，以链／值的模式进行输出。
Reduce阶段：分布式文件系统中主节点将会收集全部处理结果并整合输出结果，即原始问题的输出结果。
一般来说进行MapReduce并行计算的流程有以下五步：
1. 准备Map()输入: 先将准备输入的数据进行链／值配对。
Map input: list (k1, v1)
2. 运行用户指定Map()函数
Map output: list (k2, v2)
3. 对Map结果数据进行Shuffle操作，传输到Reduce处理器，并对相似链值同样进行Shuffle操作，将它们输入相同的Reduce处理器。
4. 运行用户指定的Reduce() 函数: 在这一阶段将会对进行Shuffle操作的数据进行用户指定Reduce()函数的操作，并产生链／值配对，准备输出。
° Reduce input: (k2, list(v2))
° Reduce output: (k3, v3)
5. 最终输出：主节点将会收集所有Reduce结果，整合并输出文档。
iii.	关键技术
1.	Hadoop项目与MapReduce算法

2.	分布式文件存储系统HDFS。
以Hadoop分布式文件系统（HDFS）为例，HDFS系统具有主／仆（Master/Slave）结构，主结点称为名字结点（Namenode），其他仆结电成为数据结点（Datanode）。主结点管理元数据，仆结点则存放和管理数据。HDFS系统可以支持成千上万的结点和上亿文件，并且具有非常好的扩展性，同时支持非结构化杂乱无章的数据。HDFS是MapReduce算法实现的底层基础。如图所示，HDFS的name node 和data node和MapReduce中的Job tracker、task tracker一一对应。
 
iv.	非关系型数据库NoSQL
众多文献表明，NoSQL可能是Hadoop项目部署最大的困难所在，传统企业应用都是SQL方式连接数据库，因此当企业选择部署大数据平台时，迁移到Hive QL和HBase之上会面临表结构变化和接口适配的问题。
 	但是实际上NoSQL是处理非结构化数据的一个很好的选择，NoSQL不再使用RDBMS模型，放弃了SQL数据库操作语句，可以在低廉的设备上支持分布式存储，同时可以很好地进行扩展节点。典型的NoSQL数据库以键/值（key-values）的方式进行数据存取，有很高的自由性。key-values是指一个键名对应一个键值，可以通过键名访问键值。例如图中所示的员工记录，各个键名对应着一个键值，可以通过键名访问键值。

 
另外一点就是NoSQL的模式相当自由，传统数据库需要提前创建表格以及相互关系，如果之后需要修改就必须按照之前设定的数据模式进行增改。而NoSQL则相当地自由，不同条目可能具有不同的属性。
v.	处理框架

b)	机器学习
i.	概述
机器学习方法指的是在计算机技术的基础上形成的学习方法，它能从数据中提炼出模型和知识。大多机器学习方法通过人工智能和动态规划等算法实现。最经典的定义是T.Mitchell在《machine learning》中提到的：“利用经验改善系统自身的性能。”它属于人工智能的一个分支，运用机器学习可以让分析员通过可得的数据集来实现决策。垃圾邮件判定、无人驾驶汽车、语音识别、金融欺诈等等方面都有机器学习的应用。
ii.	分类
1.	监督学习
2.	非监督学习
3.	强化学习
iii.	主要工具
3.	大数据分析
a)	概述。
随着大数据处理技术日渐成熟，另外一个名词浮出水面——大数据分析（Big Data Analytics）。大数据分析目前并无明确的定义，IBM公司在“What is big data analytics？”的一篇博客中描述道：“大数据分析就是高级分析技术对大规模、多样化的数据，包括结构化和非结构化以及流数据和批数据进行的应用。”SAS公司也给出了相似的定义，即大数据分析是通过对大数据进行处理从而发现隐藏规律、未知关联和其他有用信息从而辅助决策的过程。也就是说，大数据分析正是高级数据分析技术与大数据处理技术的有机结合，使得那些原先无法进入商务智能系统却又隐藏着重要细节的非结构化数据得以发挥作用的手段。值得注意和区分的是，这里所称的高级数据分析技术指的是那些在海量数据的背景下能够更好地发挥作用的技术合集（图），例如预测分析（Predictive Analytics）、数据挖掘（Data mining）、机器学习（Machine Learning）、统计分析（Statistics）、人工智能（Artificial Intelligence）、自然语言处理（Nature Language Process）等。由于大数据包含的隐藏细节更多，在数据中发现规律更为容易，我们更习惯将大数据分析称谓发现性分析（Discovery Analytics）或者探索性分析（Exploratory Analytics）。
 
b)	大数据分析生命周期
和大多数数据分析项目一样，大数据分析也有相对确定的生命周期，包括确定问题和目标、设计数据需求、数据预处理、执行分析程序以及数据可视化操作等。在Coursera.org网络课程Data Science中将生命周期划分地更为详细：
1.	确定问题
2.	确定理想的数据集
3.	明确可以操作的数据集
4.	获取数据
5.	清洗数据
6.	探索性数据分析
7.	建立描述模型／预测模型
8.	解读模型／数据可视化
9.	质疑结果
10.	得出结论
11.	撰写可复现的代码
12.	分享与反馈
遵循以上过程可以大大提高数据分析过程的效率与知识共享的效果，大数据分析其实与普通数据分析无异，需要遵循以上分析步骤，不同的是进行数据预处理和进行数据分析的时候将会用到例如Hadoop、MapReduce等工具来对非结构化的数据进行处理。
文将以《big data analytics with R and Hadoop》书中“Computing the frequency of stock market change”案例为基础，使用RHadoop包，使用R语言，应用MadReduce算法在大型集群中进行计算。本文将通过简要介绍大数据分析的整个流程，从而使读者对大数据分析或者是数据科学有一个直观的认识。
i.	明确问题
计算股价变动频率是股票市场的经典问题，可以使得投资人对不同时间段的价格变动能够有更好的洞察。所以本问题的目标就是计算股价变动百分率的频率。 
 
ii.	设计数据需求
我们可以通过Yahoo！API轻松获得股价数据。我们可以获得日期、开盘价、最高价、最低价、收盘价、成交量、调整收盘价等
R程序代码：
stock_BP <- read.csv("http://ichart.finance.yahoo.com/table.csv?s=BP")
Head(stock_BP) 
 
iii.	数据预处理
由于股价数据非常规范，所以在本例中数据预处理并不是特别需要，只需直接将数据上传至HDFS。
终端代码：
Bin/hadoop dfs –mkdir /stock
Bin/hadoop dfs –put /home Vignesh/downloads/table.csv /stock/
但是通常数据，尤其是大数据经常有缺省值、噪声以及数据不一致等等问题，需要进行数据预处理，一般包括数据清洗、数据集成、数据转换、数据归约等。实际上数据预处理占用了数据分析超过一半的时间。
iv.	进行分析
运用R和Hadoop的流处理模式，我们对数据进行分析。（本例中不需要使用到RHadoop包）
Mapper:stock-mapper.R
#关闭系统警告
Options(warn=-1)
#获取数据
Input <- file(”stdin”,”r”)
#逐行读取数据
while(length(currentLine <-readLines(input, n=1, warn=FALSE)) > 0)
{
#分解每一行
fields <- unlist(strsplit(currentLine, ","))
# 获取开放列中数据
open <- as.double(fields[2])
# 获取封闭列中数据
close <- as.double(fields[5])
# 计算开放列和封闭列数据差值
change <- (close-open)
# 排除键/值为1的数据
write(paste(change, 1, sep="\t"), stdout())
}
close(input)
Reducer: stock_reducer.R
stock.key <- NA
stock.val <- 0.0
conn <- file("stdin", open="r")
while (length(next.line <- readLines(conn, n=1)) > 0) {
split.line <- strsplit(next.line, "\t")
key <- split.line[[1]][1]
val <- as.numeric(split.line[[1]][2])
if (is.na(current.key)) {
current.key <- key
current.val <- val
}
else {
if (current.key == key) {
current.val <- current.val + val
}
else {
write(paste(current.key, current.val, sep="\t"), stdout())
current.key <- key
current.val<- val
}
}
}
write(paste(current.key, current.val, sep="\t"), stdout())
close(conn)
v.	数据可视化

c)	研究现状
d)	应用领域
e)	挑战
4.	大数据分析案例
a)	R语言
进行大数据分析有很多语言环境可共选择，而R语言是其中较为普遍的一种选择。R语言是一个进行统计分析和作图的GNU系统语言环境。其发展基于John Chambers和Allan Wilks开发的S语言（1985），最早于1995年由Robert Gentleman和Ross Ihaka开始编制,目前由R核心开发小组(RDevelopmentCoreTeam,RDCT)进行维护。不同于S语言或者后来的S-Plus，R语言是一个自由免费的开源项目，在CRAN网站上提供了5534个用户撰写的扩展包（Package）。R语言是一个完善的数学计算环境，适用非常方便，用户可以简单地调用数据库和不多的参数就可以进行分析。在RDCT小组的维护下，R语言环境提供了集成的统计工具，提供了各种数据分析、统计、机器学习等等函数，包含了从生物到金融的各种领域，被各领域的专家学者广泛使用。
不同于SPSS、SAS的统计软件，R语言有着得天独厚的优势：
1.	具有高效的数据运算和保存机制
2.	提供了一整套矩阵与数组计算操作运算符
3.	提供了完整的数据分析中间工具
4.	可以直接进行图形分析，即时得到可视化结果
5.	对不同类型的数据都具有良好的支持，可以直接对数字和非数字数据进行处理
6.	具有较好的面向对象编程（OOP）能力
7.	用户可以自由制作扩展包
8.	作为开源项目具有良好的社区支持，同时可以多种方式得到扩展包的帮助支持
基于以上的各项优点，本研究选择使用R语言来进行实验，尝试对大数据集进行探索性分析，更重要的原因，R语言作为研究学者广泛使用的数据处理工具，在中国并没有得到良好的发展，由于缺少优秀的学习课程和教材，R语言的学习曲线比较陡峭。作者希望通过本研究为R语言的普及尽一份绵薄之力。
由于R语言在处理数据时需要将所有对象读取存入虚拟内存中，所以在处理大数据集时需要使用不同的策略来解决内存不足的问题，本文将运用R语言在Hadoop集群上进行实验，所以将使用到RHadoop扩展包来使用R语言进行分布式计算。

b)	随即森林
随机森林（RF）是机器学习预测算法的一种，具有优秀的预测能力，RF属于组合分类器算法，其基本思想是利用bootsrap重抽样分类【谢益辉,朱钰. Bootstrap方法的历史发展和前沿研究[J]. 统计与信息论坛,2008,02:90-96】方法从原始空间中产生多个不同的随机子空间，对每一个不同的空间样本分别建立决策树模型，得到和样本数量一样多的预测结果，最后通过对每个结果进行投票决定最终的分类结果。详细过程见图x
 
根据Breiman.L(2001)的论文描述，随机森林无论决策树的数量如何增加，都收敛于
 
说明RF方法不会因为决策树数量的增加而带来过度拟合的困扰，但是需要注意可能产生的一定范围内的泛化误差值得一提的是，对于大样本量的时候，随着RF模型中随机特征数的增加，模型的相关系数和强度随之增加，而泛化误差反而会减少。【Breiman L^Random Forests[J]．Machine Learning，2001，45(1)．】这一点对于大数据分析来说尤为重要，这也是为什么RF模型在大数据分析的预测问题上广泛应用的原因。RF模型在十多年来得到了广泛的运用，包括在生物信息学、医学、物理学、经济管理等等领域，Coussement称在预测客户流失预测能力上，RF模型优于SVM、Logistic模型。综合来说，随机森林模型具有很好的精度，同时其运算速度远远高于同精度的AdaBoost，而且不容易产生过度拟合问题。在R语言中，随即森林算法可以通过random forest扩展包进行调用,只需要randomForest(formula, data=NULL, ..., subset, na.action=na.fail)一条语句就可以执行相关计算。在扩展包random forest的帮助文档中，简要概述了随即森林算法的优点
［http://stat-www.berkeley.edu/users/breiman/RandomForests/cc_home.htm#micro］：
1.	在现有算法中具有无与伦比的准确性
2.	在大数据集中可以高效运行
3.	可以处理上千个变量
4.	在分类中可以估计重要变量
5.	可以有效地处理缺省值，并在大量缺省值的情况下维持算法的准确性
6.	创建的森林可以储存并在以后的研究中再次使用
7.	算法可以扩展到未标签的数据和非监督聚类分析，数据展示和异常检测

c)	在线销量预测案例
d)	改写MapReduce算法-分布式随机森林算法
5.	总结
a)	内容
b)	缺陷

c)	展望
